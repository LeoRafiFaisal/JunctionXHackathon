{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dlib's face detector and landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "def get_3d_landmarks(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "        landmark_points = []\n",
    "        for n in range(0, 68):\n",
    "            x = landmarks.part(n).x\n",
    "            y = landmarks.part(n).y\n",
    "            landmark_points.append((x, y))\n",
    "        return landmark_points\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture video from a file\n",
    "cap = cv2.VideoCapture('/home/rafi-durrani/Documents/Hackathon/dataset/EAR_Selfie.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frame = cv2.flip(frame, 0)\n",
    "        landmarks = get_3d_landmarks(frame)\n",
    "        if landmarks:\n",
    "            for point in landmarks:\n",
    "                cv2.circle(frame, point, 2, (255, 0, 0), -1)\n",
    "        cv2.imshow('Frame', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_EAR(eye_points):\n",
    "    # compute the euclidean distances between the two sets of\n",
    "    # vertical eye landmarks (x, y)-coordinates\n",
    "    A = np.linalg.norm(np.array(eye_points[1]) - np.array(eye_points[5]))\n",
    "    B = np.linalg.norm(np.array(eye_points[2]) - np.array(eye_points[4]))\n",
    "\n",
    "    # compute the euclidean distance between the horizontal\n",
    "    # eye landmark (x, y)-coordinates\n",
    "    C = np.linalg.norm(np.array(eye_points[0]) - np.array(eye_points[3]))\n",
    "\n",
    "    # compute the eye aspect ratio\n",
    "    EAR = (A + B) / (2.0 * C)\n",
    "\n",
    "    return EAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mouth_open(mouth_points):\n",
    "    # Compute the euclidean distance between the upper and lower inner lips\n",
    "    if mouth_points is not None:\n",
    "        mouth_openness = np.linalg.norm(np.array(mouth_points[13]) - np.array(mouth_points[19]))  # Points 62 and 66\n",
    "        # print(mouth_openness)\n",
    "        # Define a threshold for mouth openness\n",
    "        MOUTH_OPEN_THRESHOLD = 5  # This threshold will vary depending on the individual and camera quality\n",
    "    else:\n",
    "        pass\n",
    "    return mouth_openness > MOUTH_OPEN_THRESHOLD\n",
    "\n",
    "def mouth_movement(mouth_points, previous_mouth_points):\n",
    "    if mouth_points is not None:\n",
    "    # Compute the change in mouth openness between current and previous frames\n",
    "        current_openness = np.linalg.norm(np.array(mouth_points[13]) - np.array(mouth_points[19]))\n",
    "        previous_openness = np.linalg.norm(np.array(previous_mouth_points[13]) - np.array(previous_mouth_points[19]))\n",
    "\n",
    "        # Define a threshold for determining significant mouth movement\n",
    "        MOUTH_MOVEMENT_THRESHOLD = 1  # Adjust based on your specific requirements\n",
    "        # print(abs(current_openness - previous_openness))\n",
    "    else:\n",
    "        pass\n",
    "    return abs(current_openness - previous_openness) > MOUTH_MOVEMENT_THRESHOLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(frame, landmarks):\n",
    "    # Draw facial landmarks on the frame\n",
    "    if landmarks is not None:\n",
    "        for (x, y) in landmarks:\n",
    "            cv2.circle(frame, (x, y), 1, (0, 255, 0), -1)  # Green dots\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blinking rate is abnormally low\n"
     ]
    }
   ],
   "source": [
    "# Initialize previous mouth points\n",
    "previous_mouth_points = None\n",
    "cap = cv2.VideoCapture('/home/rafi-durrani/Documents/Hackathon/dataset/EAR.avi')\n",
    "\n",
    "blink_rate = 20  # You could choose a value between 15 and 20\n",
    "# expected_blinks = average_blink_rate_per_minute * video_length_minutes\n",
    "filename  = \"output.avi\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(filename, fourcc, 20.0, (620, 430))\n",
    "EAR_THRESHOLD = 0.111\n",
    "\n",
    "MOUTH_MOVEMENT_SYNC_THRESHOLD = 5  # The threshold for lip sync discrepancies\n",
    "MOUTH_OPENING_RATE_THRESHOLD = 3  # The expected frequency of mouth openings during speech\n",
    "\n",
    "# Initialize counters and timers\n",
    "blinking_counter = 0\n",
    "mouth_open_counter = 0\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # frame = cv2.flip(frame, 0)\n",
    "        frame = cv2.resize(frame, (620, 430))\n",
    "        landmarks = get_3d_landmarks(frame)\n",
    "        \n",
    "        frame_with_landmarks = draw_landmarks(frame, landmarks)\n",
    "        \n",
    "        # Assuming landmarks is a list of (x, y) tuples\n",
    "        if landmarks is not None:\n",
    "            left_eye_EAR = calculate_EAR(landmarks[36:42])\n",
    "            right_eye_EAR = calculate_EAR(landmarks[42:48])\n",
    "            blinking = left_eye_EAR < EAR_THRESHOLD and right_eye_EAR < EAR_THRESHOLD\n",
    "\n",
    "        if blinking:\n",
    "            # print(\"Blink detected\")\n",
    "            cv2.putText(frame_with_landmarks, \"Blink detected\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            blinking_counter += 1\n",
    "\n",
    "        if previous_mouth_points is not None:\n",
    "            if landmarks is not None:\n",
    "                mouth_open = is_mouth_open(landmarks[48:68])\n",
    "                mouth_moving = mouth_movement(landmarks[48:68], previous_mouth_points)\n",
    "\n",
    "            if mouth_open:\n",
    "                # print(\"Mouth is open\")\n",
    "                cv2.putText(frame_with_landmarks, \"Mouth is open\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                mouth_open_counter += 1\n",
    "\n",
    "            if mouth_moving:\n",
    "                #  print(\"Mouth movement detected\")\n",
    "                 cv2.putText(frame_with_landmarks, \"Mouth movement detected\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        \n",
    "        \n",
    "        # Update the previous mouth points\n",
    "        if landmarks is not None:\n",
    "            previous_mouth_points = landmarks[48:68]\n",
    "\n",
    "        cv2.imshow(\"Frame\", frame_with_landmarks)\n",
    "        out.write( frame_with_landmarks)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit the video display\n",
    "            #int(1000 / (0.5 * 30))\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "# print(elapsed_time)\n",
    "# print(blinking_counter)\n",
    "if elapsed_time > 0 and (blinking_counter / elapsed_time) < blink_rate:\n",
    "    print(\"Blinking rate is abnormally low\")\n",
    "    \n",
    "if elapsed_time > 0 and (mouth_open_counter / elapsed_time) < MOUTH_OPENING_RATE_THRESHOLD:\n",
    "    print(\"Mouth opening rate during speech is abnormally low\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        landmarks = get_3d_landmarks(frame)\n",
    "        if landmarks:\n",
    "            for point in landmarks:\n",
    "                cv2.circle(frame, point, 2, (255, 0, 0), -1)\n",
    "        cv2.imshow('Frame', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... [existing code for facial landmark detection]\n",
    "\n",
    "def analyze_landmarks(landmarks):\n",
    "    # Placeholder function for analyzing landmarks\n",
    "    # You would include logic here to analyze the movements and positions\n",
    "    # of the landmarks to look for signs of deepfakes\n",
    "    pass\n",
    "\n",
    "def is_video_genuine(landmarks_list):\n",
    "    # Analyze all frames and return a confidence score or decision\n",
    "    # This is where you would integrate with a machine learning model\n",
    "    # For now, we'll use a placeholder function\n",
    "    for landmarks in landmarks_list:\n",
    "        if not analyze_landmarks(landmarks):\n",
    "            return False  # If any frame looks suspicious, flag the video\n",
    "    return True\n",
    "\n",
    "landmarks_all_frames = []\n",
    "while cap.isOpened():\n",
    "    # ... [existing frame capture code]\n",
    "    if landmarks:\n",
    "        landmarks_all_frames.append(landmarks)\n",
    "    # ... [rest of the loop]\n",
    "\n",
    "# After processing all frames\n",
    "is_genuine = is_video_genuine(landmarks_all_frames)\n",
    "print(f\"Video is {'genuine' if is_genuine else 'fake'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pointcloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
